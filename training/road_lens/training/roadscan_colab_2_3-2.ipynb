{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "EzqA9TU1LCfh",
      "metadata": {
        "id": "EzqA9TU1LCfh"
      },
      "source": [
        "# üõ£Ô∏è RoadScan AI ‚Äî Colab Training\n",
        "Run each cell top to bottom. Only edit the CONFIG cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7gYDWQlzLCfj",
      "metadata": {
        "id": "7gYDWQlzLCfj"
      },
      "source": [
        "## Cell 1 ¬∑ Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "xP5JGdzPLCfk",
      "metadata": {
        "id": "xP5JGdzPLCfk"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision pyyaml scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vlRia4JrLCfk",
      "metadata": {
        "id": "vlRia4JrLCfk"
      },
      "source": [
        "## Cell 2 ¬∑ Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cxLcp8YtLCfl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxLcp8YtLCfl",
        "outputId": "1968e166-5f90-4524-d2f7-a2a8173fa1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.10.0+cpu\n",
            "CUDA: False\n"
          ]
        }
      ],
      "source": [
        "import os, time, json\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import EfficientNet_V2_S_Weights\n",
        "from PIL import Image\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x-bDUuk9LCfl",
      "metadata": {
        "id": "x-bDUuk9LCfl"
      },
      "source": [
        "## Cell 3 ¬∑ Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "yjNiNCEaLCfl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjNiNCEaLCfl",
        "outputId": "b6ce51ef-2708-4350-829e-47e84956bcba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BW22fFMmLCfl",
      "metadata": {
        "id": "BW22fFMmLCfl"
      },
      "source": [
        "## ‚öôÔ∏è Cell 4 ¬∑ CONFIG ‚Äî Edit this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "F3GOW5oGLCfl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3GOW5oGLCfl",
        "outputId": "79d2cc61-5800-4a5f-ecce-a23874e733a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: /content/drive/MyDrive/maddata-hackathon-2026/datasets/dataset\n",
            "Output: /content/drive/MyDrive/maddata-hackathon-2026/checkpoints\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR   = \"/content/drive/MyDrive/maddata-hackathon-2026/datasets/dataset\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/maddata-hackathon-2026/checkpoints\"\n",
        "\n",
        "EPOCHS     = 20\n",
        "BATCH_SIZE = 32   # lower to 16 if out-of-memory\n",
        "LR         = 1e-3\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(\"Data:\", DATA_DIR)\n",
        "print(\"Output:\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YZdcNs1hLCfm",
      "metadata": {
        "id": "YZdcNs1hLCfm"
      },
      "source": [
        "## Cell 5 ¬∑ Load dataset.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4BnmNNa7LCfm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BnmNNa7LCfm",
        "outputId": "f9c47470-bffd-4ff5-f62f-340bfd5a10ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['potholes', 'cracked_pavement', 'road_debris_obstruction', 'broken_road_signs', 'faded_lane_markings', 'normal_road']\n",
            "Train dir: /content/drive/MyDrive/maddata-hackathon-2026/datasets/dataset/train | exists: True\n",
            "Val dir:   /content/drive/MyDrive/maddata-hackathon-2026/datasets/dataset/val | exists: True\n",
            "Test dir:  /content/drive/MyDrive/maddata-hackathon-2026/datasets/dataset/test | exists: True\n"
          ]
        }
      ],
      "source": [
        "with open(f\"{DATA_DIR}/dataset.yaml\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "CLASS_NAMES  = cfg[\"names\"]\n",
        "NUM_CLASSES  = len(CLASS_NAMES)\n",
        "class_to_idx = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "\n",
        "# Dataset uses {split}/{class_name}/ folder structure (no /images sub-folder)\n",
        "TRAIN_DIR = Path(DATA_DIR) / \"train\"\n",
        "VAL_DIR   = Path(DATA_DIR) / \"val\"\n",
        "TEST_DIR  = Path(DATA_DIR) / \"test\"\n",
        "\n",
        "print(\"Classes:\", CLASS_NAMES)\n",
        "print(\"Train dir:\", TRAIN_DIR, \"| exists:\", TRAIN_DIR.exists())\n",
        "print(\"Val dir:  \", VAL_DIR,   \"| exists:\", VAL_DIR.exists())\n",
        "print(\"Test dir: \", TEST_DIR,  \"| exists:\", TEST_DIR.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7uTTEuaqLCfm",
      "metadata": {
        "id": "7uTTEuaqLCfm"
      },
      "source": [
        "## Cell 6 ¬∑ Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "n3I7suydLCfm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3I7suydLCfm",
        "outputId": "88a93b0c-449a-49b2-b88a-027069591219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU only ‚Äî training will be slow\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU only ‚Äî training will be slow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fth1I4_VLCfm",
      "metadata": {
        "id": "Fth1I4_VLCfm"
      },
      "source": [
        "## Cell 7 ¬∑ Dataset\n",
        "\n",
        "Class is matched from the filename prefix ‚Äî longest match wins, so `broken_streetlight_abc.jpg` ‚Üí `broken_streetlight`, not `broken_sidewalk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "RJ9ogMMWLCfm",
      "metadata": {
        "id": "RJ9ogMMWLCfm"
      },
      "outputs": [],
      "source": [
        "VALID_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "\n",
        "class RoadDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Supports two dataset structures:\n",
        "\n",
        "    Standard (val / test):\n",
        "        {split_dir}/{class_name}/{image}.jpg\n",
        "\n",
        "    With synthetic sub-folder (train only):\n",
        "        {split_dir}/{class_name}/{image}.jpg          <- real images\n",
        "        {split_dir}/{class_name}/synthetic/{image}.jpg <- synthetic images\n",
        "\n",
        "    Both real and synthetic images are loaded and treated identically\n",
        "    during training. The label comes from the class_name folder, not\n",
        "    the file name or sub-folder name.\n",
        "    \"\"\"\n",
        "    def __init__(self, split_dir, transform):\n",
        "        self.transform = transform\n",
        "        self.samples   = []\n",
        "        split_path = Path(split_dir)\n",
        "\n",
        "        for class_dir in sorted(split_path.iterdir()):\n",
        "            if not class_dir.is_dir():\n",
        "                continue\n",
        "            cls = class_dir.name\n",
        "            if cls not in class_to_idx:\n",
        "                print(f\"  WARNING: folder '{cls}' not in class list ‚Äî skipping\")\n",
        "                continue\n",
        "            label = class_to_idx[cls]\n",
        "            real, synth = 0, 0\n",
        "\n",
        "            for item in sorted(class_dir.iterdir()):\n",
        "                if item.is_file() and item.suffix.lower() in VALID_EXTS:\n",
        "                    # Direct image inside class folder (real data)\n",
        "                    self.samples.append((item, label))\n",
        "                    real += 1\n",
        "                elif item.is_dir() and item.name == \"synthetic\":\n",
        "                    # Recurse one level into synthetic/ sub-folder\n",
        "                    for img_path in sorted(item.iterdir()):\n",
        "                        if img_path.suffix.lower() in VALID_EXTS:\n",
        "                            self.samples.append((img_path, label))\n",
        "                            synth += 1\n",
        "\n",
        "            print(f\"  [{cls}]  real={real}  synthetic={synth}  total={real+synth}\")\n",
        "\n",
        "        print(f\"  ‚Üí {len(self.samples)} total images from {split_path.name}/\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        return self.transform(Image.open(path).convert(\"RGB\")), label\n",
        "\n",
        "    @property\n",
        "    def targets(self):\n",
        "        return [lbl for _, lbl in self.samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cE1-6rQCLCfn",
      "metadata": {
        "id": "cE1-6rQCLCfn"
      },
      "source": [
        "## Cell 8 ¬∑ Transforms & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "K7MGBeu1LCfn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7MGBeu1LCfn",
        "outputId": "3fd15a59-653f-4647-a3b9-2664beac8b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train dataset...\n",
            "  [broken_road_signs]  real=960  synthetic=29  total=989\n",
            "  [cracked_pavement]  real=960  synthetic=63  total=1023\n",
            "  [faded_lane_markings]  real=960  synthetic=28  total=988\n",
            "  [normal_road]  real=960  synthetic=51  total=1011\n",
            "  [potholes]  real=960  synthetic=61  total=1021\n",
            "  [road_debris_obstruction]  real=960  synthetic=68  total=1028\n",
            "  ‚Üí 6060 total images from train/\n",
            "Loading val dataset...\n",
            "  [broken_road_signs]  real=120  synthetic=0  total=120\n",
            "  [cracked_pavement]  real=120  synthetic=0  total=120\n",
            "  [faded_lane_markings]  real=120  synthetic=0  total=120\n",
            "  [normal_road]  real=120  synthetic=0  total=120\n",
            "  [potholes]  real=120  synthetic=0  total=120\n",
            "  [road_debris_obstruction]  real=120  synthetic=0  total=120\n",
            "  ‚Üí 720 total images from val/\n",
            "\n",
            "Class distribution (train):\n",
            "  [0] potholes                 : 1021 images\n",
            "  [1] cracked_pavement         : 1023 images\n",
            "  [2] road_debris_obstruction  : 1028 images\n",
            "  [3] broken_road_signs        : 989 images\n",
            "  [4] faded_lane_markings      : 988 images\n",
            "  [5] normal_road              : 1011 images\n"
          ]
        }
      ],
      "source": [
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE + 24, IMAGE_SIZE + 24)),\n",
        "    transforms.RandomCrop(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "print(\"Loading train dataset...\")\n",
        "train_ds = RoadDataset(TRAIN_DIR, train_tf)\n",
        "print(\"Loading val dataset...\")\n",
        "val_ds   = RoadDataset(VAL_DIR, val_tf)\n",
        "\n",
        "# Weighted sampler to handle any remaining class imbalance\n",
        "targets     = torch.tensor(train_ds.targets)\n",
        "class_count = torch.bincount(targets, minlength=NUM_CLASSES).float()\n",
        "class_count[class_count == 0] = 1\n",
        "weights     = 1.0 / class_count[targets]\n",
        "sampler     = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "\n",
        "NUM_WORKERS  = 2 if device.type == \"cuda\" else 0\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"\\nClass distribution (train):\")\n",
        "for i, cls in enumerate(CLASS_NAMES):\n",
        "    print(f\"  [{i}] {cls:<25s}: {int(class_count[i])} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VegJniYPLCfn",
      "metadata": {
        "id": "VegJniYPLCfn"
      },
      "source": [
        "## Cell 9 ¬∑ Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "awNQbJ1oLCfn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awNQbJ1oLCfn",
        "outputId": "d5bce47e-5cfc-49ff-8594-af6926bf0fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82.7M/82.7M [00:00<00:00, 117MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready ‚Äî trainable params: 329478\n"
          ]
        }
      ],
      "source": [
        "model = models.efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze backbone ‚Äî train head only during warm-up\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier for our classes\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(256, NUM_CLASSES),\n",
        ")\n",
        "model = model.to(device)\n",
        "print(\"Model ready ‚Äî trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aTdM3CohLCfn",
      "metadata": {
        "id": "aTdM3CohLCfn"
      },
      "source": [
        "## Cell 10 ¬∑ Train\n",
        "\n",
        "- **Epochs 1‚Äì3 (warm-up):** only the classifier head trains, backbone frozen.\n",
        "- **Epoch 4+:** last backbone blocks unfreeze for full fine-tuning.\n",
        "- Stops early if val accuracy doesn't improve for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Omr2CiQWLCfn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omr2CiQWLCfn",
        "outputId": "db87e01a-743d-4e67-de40-24272fe4d34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/20 | Train loss 0.6038 acc 0.801 | Val loss 0.3732 acc 0.886 | 2768.4s\n",
            "  üíæ Saved best model (val_acc=0.8861)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02/20 | Train loss 0.4105 acc 0.860 | Val loss 0.3257 acc 0.887 | 1976.4s\n",
            "  üíæ Saved best model (val_acc=0.8875)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03/20 | Train loss 0.3637 acc 0.877 | Val loss 0.3019 acc 0.886 | 1772.9s\n",
            "\n",
            "[Phase 2] Unfreezing backbone blocks 6+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 04/20 [Train]:  16%|‚ñà‚ñå        | 30/190 [05:05<28:09, 10.56s/it, acc=0.866, loss=0.3784]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler    = torch.amp.GradScaler(\"cuda\") if device.type == \"cuda\" else None\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "WARMUP   = 3\n",
        "PATIENCE = 5\n",
        "best_acc = 0.0\n",
        "no_imp   = 0\n",
        "history  = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "ckpt_path = Path(OUTPUT_DIR) / \"best_roadscan(latest).pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Unfreeze backbone after warm-up\n",
        "    if epoch == WARMUP + 1:\n",
        "        print(\"\\n[Phase 2] Unfreezing backbone blocks 6+\")\n",
        "        for i, layer in enumerate(model.features):\n",
        "            if i >= 6:\n",
        "                for p in layer.parameters():\n",
        "                    p.requires_grad = True\n",
        "        optimizer = optim.AdamW([\n",
        "            {\"params\": model.features.parameters(),   \"lr\": LR * 0.1},\n",
        "            {\"params\": model.classifier.parameters(), \"lr\": LR},\n",
        "        ], weight_decay=1e-4)\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS - epoch)\n",
        "\n",
        "    # ‚îÄ‚îÄ Train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    model.train()\n",
        "    tr_loss, tr_correct, tr_total = 0.0, 0, 0\n",
        "\n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch:02d}/{EPOCHS} [Train]\", leave=False)\n",
        "    for imgs, labels in train_bar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                out  = model(imgs)\n",
        "                loss = criterion(out, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        tr_loss    += loss.item() * imgs.size(0)\n",
        "        tr_correct += (out.argmax(1) == labels).sum().item()\n",
        "        tr_total   += imgs.size(0)\n",
        "        train_bar.set_postfix(loss=f\"{tr_loss/tr_total:.4f}\", acc=f\"{tr_correct/tr_total:.3f}\")\n",
        "\n",
        "    # ‚îÄ‚îÄ Validate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    model.eval()\n",
        "    vl_loss, vl_correct, vl_total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch:02d}/{EPOCHS} [Val]  \", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_bar:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out   = model(imgs)\n",
        "            loss  = criterion(out, labels)\n",
        "            preds = out.argmax(1)\n",
        "\n",
        "            vl_loss    += loss.item() * imgs.size(0)\n",
        "            vl_correct += (preds == labels).sum().item()\n",
        "            vl_total   += imgs.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            val_bar.set_postfix(loss=f\"{vl_loss/vl_total:.4f}\", acc=f\"{vl_correct/vl_total:.3f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    tr_acc = tr_correct / tr_total\n",
        "    vl_acc = vl_correct / vl_total\n",
        "    history[\"train_loss\"].append(tr_loss / tr_total)\n",
        "    history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(vl_loss / vl_total)\n",
        "    history[\"val_acc\"].append(vl_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{EPOCHS} | Train loss {tr_loss/tr_total:.4f} acc {tr_acc:.3f} | Val loss {vl_loss/vl_total:.4f} acc {vl_acc:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # ‚îÄ‚îÄ Save progress after every epoch (safe against Colab crashes) ‚îÄ\n",
        "    with open(Path(OUTPUT_DIR) / \"history.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            \"epoch\":      epoch,\n",
        "            \"best_acc\":   best_acc,\n",
        "            \"history\":    history,\n",
        "            \"epochs_log\": [\n",
        "                {\n",
        "                    \"epoch\":      e + 1,\n",
        "                    \"train_loss\": round(history[\"train_loss\"][e], 4),\n",
        "                    \"train_acc\":  round(history[\"train_acc\"][e],  4),\n",
        "                    \"val_loss\":   round(history[\"val_loss\"][e],   4),\n",
        "                    \"val_acc\":    round(history[\"val_acc\"][e],    4),\n",
        "                }\n",
        "                for e in range(len(history[\"train_acc\"]))\n",
        "            ],\n",
        "        }, f, indent=2)\n",
        "\n",
        "    if vl_acc > best_acc:\n",
        "        best_acc = vl_acc\n",
        "        no_imp   = 0\n",
        "        torch.save({\"model_state\": model.state_dict(), \"classes\": CLASS_NAMES,\n",
        "                    \"class_to_idx\": class_to_idx, \"image_size\": IMAGE_SIZE}, ckpt_path)\n",
        "        print(f\"  üíæ Saved best model (val_acc={vl_acc:.4f})\")\n",
        "    else:\n",
        "        no_imp += 1\n",
        "        if no_imp >= PATIENCE:\n",
        "            print(f\"\\n‚èπÔ∏è  Early stopping ‚Äî no improvement for {PATIENCE} epochs\")\n",
        "            break\n",
        "\n",
        "print(f\"\\n‚úÖ Done. Best val acc: {best_acc:.4f}\")\n",
        "print(f\"   Checkpoint: {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print confidence distribution across val set\n",
        "all_confs = []\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in val_loader:\n",
        "        probs = torch.softmax(model(imgs.to(device)), dim=-1)\n",
        "        all_confs.extend(probs.max(dim=-1).values.cpu().tolist())\n",
        "\n",
        "all_confs = sorted(all_confs)\n",
        "print(f\"Val confidence ‚Äî p10={all_confs[len(all_confs)//10]:.3f}  \"\n",
        "      f\"p50={all_confs[len(all_confs)//2]:.3f}  \"\n",
        "      f\"p90={all_confs[int(len(all_confs)*0.9)]:.3f}\")"
      ],
      "metadata": {
        "id": "5uy-pZZrM8t1"
      },
      "id": "5uy-pZZrM8t1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ZN8PzJP4LCfo",
      "metadata": {
        "id": "ZN8PzJP4LCfo"
      },
      "source": [
        "## Cell 11 ¬∑ Per-Class Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kJdkqifbLCfo",
      "metadata": {
        "id": "kJdkqifbLCfo"
      },
      "outputs": [],
      "source": [
        "## Cell 11 ¬∑ Per-Class Report\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=3))\n",
        "\n",
        "# ‚îÄ‚îÄ Confidence-gated prediction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "NO_ISSUE_THRESHOLD = 0.60   # below this ‚Üí \"No Issue Detected\"\n",
        "REJECT_THRESHOLD   = 0.85   # below this ‚Üí unknown/OOD\n",
        "\n",
        "def predict_with_rejection(model, input_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits     = model(input_tensor)\n",
        "        probs      = torch.softmax(logits, dim=-1)\n",
        "        confidence, pred = probs.max(dim=-1)\n",
        "\n",
        "    conf = confidence.item()\n",
        "\n",
        "    if conf < NO_ISSUE_THRESHOLD:\n",
        "        return {\"label\": \"No Issue Detected\", \"confidence\": conf}\n",
        "    elif conf < REJECT_THRESHOLD:\n",
        "        return {\"label\": \"UNKNOWN / Out-of-Distribution\", \"confidence\": conf}\n",
        "    else:\n",
        "        return {\"label\": CLASS_NAMES[pred.item()], \"confidence\": conf}\n",
        "\n",
        "# Quick sanity-check on the val set ‚Äî prints a few sample predictions\n",
        "print(\"\\nSample predictions with confidence gating:\")\n",
        "print(f\"  < {NO_ISSUE_THRESHOLD:.0%}  ‚Üí No Issue Detected\")\n",
        "print(f\"  < {REJECT_THRESHOLD:.0%}  ‚Üí UNKNOWN / OOD\")\n",
        "print(f\"  ‚â• {REJECT_THRESHOLD:.0%}  ‚Üí Predicted class\")\n",
        "print()\n",
        "\n",
        "model.eval()\n",
        "val_tf_single = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "shown = 0\n",
        "for class_dir in sorted((Path(DATA_DIR) / \"val\").iterdir()):\n",
        "    if not class_dir.is_dir() or shown >= 6:\n",
        "        break\n",
        "    for img_path in sorted(class_dir.iterdir()):\n",
        "        if img_path.suffix.lower() not in {\".jpg\", \".jpeg\", \".png\"}:\n",
        "            continue\n",
        "        tensor = val_tf_single(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "        result = predict_with_rejection(model, tensor)\n",
        "        true_label = class_dir.name\n",
        "        match = \"‚úì\" if result[\"label\"] == true_label else \"‚úó\"\n",
        "        print(f\"  {match} true={true_label:<25s}  pred={result['label']:<30s}  conf={result['confidence']:.3f}\")\n",
        "        shown += 1\n",
        "        break\n",
        "\"\"\"\n",
        "The two thresholds explained:\n",
        "\n",
        "conf < 0.60  ‚Üí  \"No Issue Detected\"\n",
        "               Model isn't sure enough to flag anything ‚Äî treat the road as fine.\n",
        "               This is your safe-pass gate.\n",
        "\n",
        "0.60 ‚â§ conf < 0.85  ‚Üí  \"UNKNOWN / OOD\"\n",
        "               Model sees *something* but can't commit ‚Äî likely a weird angle,\n",
        "               lighting condition, or something genuinely outside training data.\n",
        "\n",
        "conf ‚â• 0.85  ‚Üí  Predicted class name\n",
        "               High-confidence ‚Äî use the result.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O8-4J4ZaLCfo",
      "metadata": {
        "id": "O8-4J4ZaLCfo"
      },
      "source": [
        "## Cell 12 ¬∑ Plot Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85q8AZ_fLCfo",
      "metadata": {
        "id": "85q8AZ_fLCfo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax1.plot(history[\"train_loss\"], label=\"Train\"); ax1.plot(history[\"val_loss\"], label=\"Val\")\n",
        "ax1.set_title(\"Loss\"); ax1.set_xlabel(\"Epoch\"); ax1.legend(); ax1.grid(True)\n",
        "ax2.plot(history[\"train_acc\"], label=\"Train\"); ax2.plot(history[\"val_acc\"], label=\"Val\")\n",
        "ax2.set_title(\"Accuracy\"); ax2.set_xlabel(\"Epoch\"); ax2.legend(); ax2.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(OUTPUT_DIR) / \"training_curves.png\", dpi=120)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ps_n4PmI-dJb",
      "metadata": {
        "id": "Ps_n4PmI-dJb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}