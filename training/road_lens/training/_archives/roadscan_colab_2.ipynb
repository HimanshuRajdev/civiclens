{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "EzqA9TU1LCfh",
      "metadata": {
        "id": "EzqA9TU1LCfh"
      },
      "source": [
        "# üõ£Ô∏è RoadScan AI ‚Äî Colab Training\n",
        "Run each cell top to bottom. Only edit the CONFIG cell."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7gYDWQlzLCfj",
      "metadata": {
        "id": "7gYDWQlzLCfj"
      },
      "source": [
        "## Cell 1 ¬∑ Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "xP5JGdzPLCfk",
      "metadata": {
        "id": "xP5JGdzPLCfk"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision pyyaml scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vlRia4JrLCfk",
      "metadata": {
        "id": "vlRia4JrLCfk"
      },
      "source": [
        "## Cell 2 ¬∑ Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cxLcp8YtLCfl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxLcp8YtLCfl",
        "outputId": "479855d8-0314-4f12-8143-1a35d0ee8539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.10.0+cpu\n",
            "CUDA: False\n"
          ]
        }
      ],
      "source": [
        "import os, time, json\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import EfficientNet_V2_S_Weights\n",
        "from PIL import Image\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x-bDUuk9LCfl",
      "metadata": {
        "id": "x-bDUuk9LCfl"
      },
      "source": [
        "## Cell 3 ¬∑ Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "yjNiNCEaLCfl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjNiNCEaLCfl",
        "outputId": "f28ac723-2f59-442b-f5ad-8f45aafb97fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BW22fFMmLCfl",
      "metadata": {
        "id": "BW22fFMmLCfl"
      },
      "source": [
        "## ‚öôÔ∏è Cell 4 ¬∑ CONFIG ‚Äî Edit this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "F3GOW5oGLCfl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3GOW5oGLCfl",
        "outputId": "b18ac2b7-6abf-4dff-b5a8-c95a653fd101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: /content/drive/MyDrive/maddata-hackathon-2026/datasets/final\n",
            "Output: /content/drive/MyDrive/maddata-hackathon-2026/checkpoints\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR   = \"/content/drive/MyDrive/maddata-hackathon-2026/datasets/final\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/maddata-hackathon-2026/checkpoints\"\n",
        "\n",
        "EPOCHS     = 20\n",
        "BATCH_SIZE = 32   # lower to 16 if out-of-memory\n",
        "LR         = 1e-3\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(\"Data:\", DATA_DIR)\n",
        "print(\"Output:\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YZdcNs1hLCfm",
      "metadata": {
        "id": "YZdcNs1hLCfm"
      },
      "source": [
        "## Cell 5 ¬∑ Load dataset.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4BnmNNa7LCfm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BnmNNa7LCfm",
        "outputId": "a3e28545-601b-4faf-cd36-0988464d9162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['pothole', 'garbage_overflow', 'broken_streetlight', 'water_leakage', 'broken_sidewalk', 'sinkhole']\n",
            "Train dir: /content/drive/MyDrive/maddata-hackathon-2026/datasets/final/train/images | exists: True\n",
            "Val dir:   /content/drive/MyDrive/maddata-hackathon-2026/datasets/final/val/images | exists: True\n"
          ]
        }
      ],
      "source": [
        "with open(f\"{DATA_DIR}/dataset.yaml\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "CLASS_NAMES  = cfg[\"names\"]\n",
        "NUM_CLASSES  = len(CLASS_NAMES)\n",
        "class_to_idx = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "\n",
        "TRAIN_IMG_DIR = Path(DATA_DIR) / \"train\" / \"images\"\n",
        "VAL_IMG_DIR   = Path(DATA_DIR) / \"val\"   / \"images\"\n",
        "\n",
        "print(\"Classes:\", CLASS_NAMES)\n",
        "print(\"Train dir:\", TRAIN_IMG_DIR, \"| exists:\", TRAIN_IMG_DIR.exists())\n",
        "print(\"Val dir:  \", VAL_IMG_DIR,   \"| exists:\", VAL_IMG_DIR.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7uTTEuaqLCfm",
      "metadata": {
        "id": "7uTTEuaqLCfm"
      },
      "source": [
        "## Cell 6 ¬∑ Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "n3I7suydLCfm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3I7suydLCfm",
        "outputId": "cbfdfbbd-83e1-41e5-d91e-a4a099f68b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU only ‚Äî training will be slow\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU only ‚Äî training will be slow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fth1I4_VLCfm",
      "metadata": {
        "id": "Fth1I4_VLCfm"
      },
      "source": [
        "## Cell 7 ¬∑ Dataset\n",
        "\n",
        "Class is matched from the filename prefix ‚Äî longest match wins, so `broken_streetlight_abc.jpg` ‚Üí `broken_streetlight`, not `broken_sidewalk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "RJ9ogMMWLCfm",
      "metadata": {
        "id": "RJ9ogMMWLCfm"
      },
      "outputs": [],
      "source": [
        "VALID_EXTS    = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "sorted_classes = sorted(CLASS_NAMES, key=len, reverse=True)  # longest first\n",
        "\n",
        "def match_class(stem):\n",
        "    for cls in sorted_classes:\n",
        "        if stem == cls or stem.startswith(cls + \"_\"):\n",
        "            return cls\n",
        "    return None\n",
        "\n",
        "class RoadDataset(Dataset):\n",
        "    def __init__(self, images_dir, transform):\n",
        "        self.transform = transform\n",
        "        self.samples   = []\n",
        "        skipped = 0\n",
        "        for p in sorted(Path(images_dir).iterdir()):\n",
        "            if p.suffix.lower() not in VALID_EXTS:\n",
        "                continue\n",
        "            cls = match_class(p.stem)\n",
        "            if cls is not None:\n",
        "                self.samples.append((p, class_to_idx[cls]))\n",
        "            else:\n",
        "                skipped += 1\n",
        "        print(f\"  {len(self.samples)} images loaded, {skipped} skipped\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        return self.transform(Image.open(path).convert(\"RGB\")), label\n",
        "\n",
        "    @property\n",
        "    def targets(self):\n",
        "        return [lbl for _, lbl in self.samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cE1-6rQCLCfn",
      "metadata": {
        "id": "cE1-6rQCLCfn"
      },
      "source": [
        "## Cell 8 ¬∑ Transforms & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "K7MGBeu1LCfn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7MGBeu1LCfn",
        "outputId": "146b208a-0918-4d8c-fb8c-ff72cbafd937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading train dataset...\n",
            "  11356 images loaded, 0 skipped\n",
            "Loading val dataset...\n",
            "  3216 images loaded, 0 skipped\n",
            "\n",
            "Class distribution (train):\n",
            "  [0] pothole                  : 1303 images\n",
            "  [1] garbage_overflow         : 1961 images\n",
            "  [2] broken_streetlight       : 6927 images\n",
            "  [3] water_leakage            : 61 images\n",
            "  [4] broken_sidewalk          : 183 images\n",
            "  [5] sinkhole                 : 921 images\n"
          ]
        }
      ],
      "source": [
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE + 24, IMAGE_SIZE + 24)),\n",
        "    transforms.RandomCrop(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "print(\"Loading train dataset...\")\n",
        "train_ds = RoadDataset(TRAIN_IMG_DIR, train_tf)\n",
        "print(\"Loading val dataset...\")\n",
        "val_ds   = RoadDataset(VAL_IMG_DIR, val_tf)\n",
        "\n",
        "# Weighted sampler to balance class imbalance\n",
        "targets     = torch.tensor(train_ds.targets)\n",
        "class_count = torch.bincount(targets, minlength=NUM_CLASSES).float()\n",
        "class_count[class_count == 0] = 1\n",
        "weights     = 1.0 / class_count[targets]\n",
        "sampler     = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "\n",
        "NUM_WORKERS  = 2 if device.type == \"cuda\" else 0\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"\\nClass distribution (train):\")\n",
        "for i, cls in enumerate(CLASS_NAMES):\n",
        "    print(f\"  [{i}] {cls:<25s}: {int(class_count[i])} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VegJniYPLCfn",
      "metadata": {
        "id": "VegJniYPLCfn"
      },
      "source": [
        "## Cell 9 ¬∑ Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "awNQbJ1oLCfn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awNQbJ1oLCfn",
        "outputId": "1bb3bca3-08fd-4e81-f60d-bcd160cad3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model ready ‚Äî trainable params: 329478\n"
          ]
        }
      ],
      "source": [
        "model = models.efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze backbone ‚Äî train head only during warm-up\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier for our classes\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(256, NUM_CLASSES),\n",
        ")\n",
        "model = model.to(device)\n",
        "print(\"Model ready ‚Äî trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aTdM3CohLCfn",
      "metadata": {
        "id": "aTdM3CohLCfn"
      },
      "source": [
        "## Cell 10 ¬∑ Train\n",
        "\n",
        "- **Epochs 1‚Äì3 (warm-up):** only the classifier head trains, backbone frozen.\n",
        "- **Epoch 4+:** last backbone blocks unfreeze for full fine-tuning.\n",
        "- Stops early if val accuracy doesn't improve for 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Omr2CiQWLCfn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omr2CiQWLCfn",
        "outputId": "820ab769-3f23-4a33-ee26-4d3148e0229f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 01/20 [Train]:   0%|          | 0/355 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "Epoch 01/20 [Train]:   1%|          | 3/355 [01:04<2:03:54, 21.12s/it, acc=0.240, loss=1.7563]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler    = torch.amp.GradScaler(\"cuda\") if device.type == \"cuda\" else None\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "WARMUP   = 3\n",
        "PATIENCE = 5\n",
        "best_acc = 0.0\n",
        "no_imp   = 0\n",
        "history  = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "ckpt_path = Path(OUTPUT_DIR) / \"best_roadscan.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Unfreeze backbone after warm-up\n",
        "    if epoch == WARMUP + 1:\n",
        "        print(\"\\n[Phase 2] Unfreezing backbone blocks 6+\")\n",
        "        for i, layer in enumerate(model.features):\n",
        "            if i >= 6:\n",
        "                for p in layer.parameters():\n",
        "                    p.requires_grad = True\n",
        "        optimizer = optim.AdamW([\n",
        "            {\"params\": model.features.parameters(),   \"lr\": LR * 0.1},\n",
        "            {\"params\": model.classifier.parameters(), \"lr\": LR},\n",
        "        ], weight_decay=1e-4)\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS - epoch)\n",
        "\n",
        "    # ‚îÄ‚îÄ Train ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    model.train()\n",
        "    tr_loss, tr_correct, tr_total = 0.0, 0, 0\n",
        "\n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch:02d}/{EPOCHS} [Train]\", leave=False)\n",
        "    for imgs, labels in train_bar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            with torch.amp.autocast(\"cuda\"):\n",
        "                out  = model(imgs)\n",
        "                loss = criterion(out, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        tr_loss    += loss.item() * imgs.size(0)\n",
        "        tr_correct += (out.argmax(1) == labels).sum().item()\n",
        "        tr_total   += imgs.size(0)\n",
        "\n",
        "        train_bar.set_postfix(loss=f\"{tr_loss/tr_total:.4f}\", acc=f\"{tr_correct/tr_total:.3f}\")\n",
        "\n",
        "    # ‚îÄ‚îÄ Validate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    model.eval()\n",
        "    vl_loss, vl_correct, vl_total = 0.0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch:02d}/{EPOCHS} [Val]  \", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_bar:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out   = model(imgs)\n",
        "            loss  = criterion(out, labels)\n",
        "            preds = out.argmax(1)\n",
        "\n",
        "            vl_loss    += loss.item() * imgs.size(0)\n",
        "            vl_correct += (preds == labels).sum().item()\n",
        "            vl_total   += imgs.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            val_bar.set_postfix(loss=f\"{vl_loss/vl_total:.4f}\", acc=f\"{vl_correct/vl_total:.3f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    tr_acc = tr_correct / tr_total\n",
        "    vl_acc = vl_correct / vl_total\n",
        "    history[\"train_loss\"].append(tr_loss / tr_total)\n",
        "    history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(vl_loss / vl_total)\n",
        "    history[\"val_acc\"].append(vl_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{EPOCHS} | Train loss {tr_loss/tr_total:.4f} acc {tr_acc:.3f} | Val loss {vl_loss/vl_total:.4f} acc {vl_acc:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    if vl_acc > best_acc:\n",
        "        best_acc = vl_acc\n",
        "        no_imp   = 0\n",
        "        torch.save({\"model_state\": model.state_dict(), \"classes\": CLASS_NAMES,\n",
        "                    \"class_to_idx\": class_to_idx, \"image_size\": IMAGE_SIZE}, ckpt_path)\n",
        "        print(f\"  üíæ Saved best model (val_acc={vl_acc:.4f})\")\n",
        "    else:\n",
        "        no_imp += 1\n",
        "        if no_imp >= PATIENCE:\n",
        "            print(f\"\\n‚èπÔ∏è  Early stopping ‚Äî no improvement for {PATIENCE} epochs\")\n",
        "            break\n",
        "\n",
        "with open(Path(OUTPUT_DIR) / \"history.json\", \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Done. Best val acc: {best_acc:.4f}\")\n",
        "print(f\"   Checkpoint: {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZN8PzJP4LCfo",
      "metadata": {
        "id": "ZN8PzJP4LCfo"
      },
      "source": [
        "## Cell 11 ¬∑ Per-Class Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kJdkqifbLCfo",
      "metadata": {
        "id": "kJdkqifbLCfo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O8-4J4ZaLCfo",
      "metadata": {
        "id": "O8-4J4ZaLCfo"
      },
      "source": [
        "## Cell 12 ¬∑ Plot Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85q8AZ_fLCfo",
      "metadata": {
        "id": "85q8AZ_fLCfo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax1.plot(history[\"train_loss\"], label=\"Train\"); ax1.plot(history[\"val_loss\"], label=\"Val\")\n",
        "ax1.set_title(\"Loss\"); ax1.set_xlabel(\"Epoch\"); ax1.legend(); ax1.grid(True)\n",
        "ax2.plot(history[\"train_acc\"], label=\"Train\"); ax2.plot(history[\"val_acc\"], label=\"Val\")\n",
        "ax2.set_title(\"Accuracy\"); ax2.set_xlabel(\"Epoch\"); ax2.legend(); ax2.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(OUTPUT_DIR) / \"training_curves.png\", dpi=120)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
